---
title: "Mixture of Gaussians and the EM Algorithm"
author: "Silvana Alvarez - Sergio Quntanilla"
date: "2025-03-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Point 1

Implement the EM algorithm to estimate the parameters of a mixture of Gaussians when we have data of any dimension and any number of classes.

**Solution:**

```{r}
source(file = "EM_function.R")
library(ggplot2)
library(mvtnorm)
library(reshape)
library(tidyverse)
```

```{r}
#Example usage:
library(MASS)  # For generating multivariate normal data

# Generate some example data
set.seed(123)
n1 <- 200
n2 <- 150
X1 <- mvrnorm(n1, mu = c(0, 0), Sigma = matrix(c(1, 0, 0, 1), 2, 2))
X2 <- mvrnorm(n2, mu = c(5, 5), Sigma = matrix(c(1, 0.5, 0.5, 1), 2, 2))
X <- rbind(X1, X2)

# Set initial parameters
K <- 2
D <- 2

# Run EM algorithm with initial parameters
model <- em_gaussian_mixture(X, K, max_iter = 50)

# Plot results
plot(X, col = predict_cluster(model, X), pch = 19)
points(model$mu, col = 1:model$K, pch = 8, cex = 2)
plot_convergence(model)
```

# Point 2

Check that it works for synthetic data generated according to a mixture of Gaussians in:

## a) 1 dimensions

**Solution:**
```{r}
Mu1 = 5
Mu2 = 7
S1 = 1
S2 = 2

pi = 1/3
n = 300

set.seed(100)
data = matrix(0, n, 1)
z = rep(0,n)
for (i in 1:n){
  z[i] = rbinom(1,1,pi)
  if (z[i] ==1){
    data[i,] = rnorm(1, Mu1,S1)
  }else{
    data[i,] = rnorm(1, Mu2,S2)
  }
}

to.plot = data.frame(x = data[,1], 
                     class =  as.factor(z))
ggplot(to.plot) + geom_density(aes(x=x,fill=class), alpha=.5) + geom_density(aes(x=x))
```

The black line through the two distributions shows the mixed distribution curve.

```{r,warning=FALSE}
X=data
K=2
# Run EM algorithm with initial parameters
model_1d <- em_gaussian_mixture(X, K, max_iter = 50)


# Plot results
to.plot = data.frame(x = data[,1],
                     class = as.factor(predict_cluster(model_1d, X)))

# hist(X, col = predict_cluster(model_1d, X), pch = 19)
ggplot(to.plot) + geom_density(aes(x=x,fill=class), alpha=.5)+
  geom_vline(aes(xintercept = model_1d$mu[1]), color = "red", linetype = "dashed", size = 1)+
  geom_vline(aes(xintercept = model_1d$mu[2]), color = "blue", linetype = "dashed", size = 1)

plot_convergence(model_1d)

to.plot = data.frame(x = data[,1],
                     iter = model_1d$Alpha[,1:8])
to.plot <- pivot_longer(to.plot, cols = -c("x"))

ggplot(to.plot)+aes(x, color = value)+
  geom_histogram()+facet_wrap(~name, nrow = 2)

```











## b) 2 dimensions

**Solution:**

test for 3 dists
```{r}
library(ggplot2)
library(mvtnorm)
Mu1 = c(1,1)
Mu2 = c(7,7)
Mu3 = c(3,3)
Sigma1 = matrix(c(2, 1, 1, 1), 2,2)
Sigma2 = matrix(c(2, 2, 2, 5), 2,2)
Sigma3 = matrix(c(3, 2, 2, 3), 2,2)

pi = 1/3
n = 300

set.seed(100)
data = matrix(0, n, 2)
z = rep(0,n)
for (i in 1:n){
  z[i] = runif(1) #generate a random val to determine from what dist it comes
  if (z[i] <= 0.33){
    data[i,] = rmvnorm(1, Mu1,Sigma1)
    z[i] = 0
  }else if (z[i] >= 0.67){
    data[i,] = rmvnorm(1, Mu2,Sigma2)
    z[i] = 1
  }else {
    data[i,] = rmvnorm(1, Mu3,Sigma3)
    z[i] = 2
  }
}

to.plot = data.frame(x = data[,1], 
                     y = data[,2], 
                     class =  as.factor(z))
ggplot(to.plot)+ aes(x, y, color = class)+
  geom_point()+geom_density_2d()

```

```{r}
X=data
K=3
# Run EM algorithm with initial parameters
model_2d <- em_gaussian_mixture(X, K, max_iter = 50)

# Plot results
plot(X, col = predict_cluster(model_2d, X), pch = 19)
points(model_2d$mu, col = 1:model_2d$K, pch = 8, cex = 2)
plot_convergence(model_2d)
```


```{r}
to.plot = data.frame(x = data[,1],
                     y = data[,2],
                     iter = model_2d$Alpha[,1:8])

to.plot = melt(to.plot, id.vars = c("x", "y"))
to.plot
# model_2d$Alpha
ggplot(to.plot)+aes(x,y, color = value)+
  geom_point()+facet_wrap(~variable, nrow = 2)
```




## c) 3 dimensions

**Solution:**


# Point 3

Once you know the algorithm works, apply it to segment three images where you think there are different classes and show the result.




