k = best_params$k,
percent_threshold = best_params$percent,
n_components = best_params$n_comp,
distance_function = best_params$distance,
accuracy = best_params$accuracy,
threshold_value = best_threshold,
variance_explained = best_var_exp
))
}
n_comp_thresholds <- c(0.80, 0.85, 0.90, 0.95)  # Cumulative variance thresholds for PCA
k_values <- c(1, 3, 5)  # Number of neighbors for KNN
percent_thresholds <- c(0.1, 0.2, 0.3)  # Thresholds for novel person detection
num_splits <- 5  # Number of cross-validation splits
num_persons_out <- 2  # Number of persons to leave out for testing
PCA_model = PCA(X,21)
dim(PCA_model$vectors)
PCA_model = PCA(X,21)
dim(PCA_model$vectors)
dim(PCA_model$values)
length(PCA_model$values)
PCA
proj = project_pca(X,PCA_model)[,1:21]
# Run the tuning function
source("function_loading.R")
n_comp_thresholds <- c(0.80, 0.85, 0.90, 0.95)  # Cumulative variance thresholds for PCA
k_values <- c(1, 3, 5)  # Number of neighbors for KNN
percent_thresholds <- c(0.1, 0.2, 0.3)  # Thresholds for novel person detection
num_splits <- 5  # Number of cross-validation splits
num_persons_out <- 2  # Number of persons to leave out for testing
tuning_results_fisher <- lppo_tuning_FISHER(
data = proj,
labels = labels,
num_persons_out = num_persons_out,
n_comp_thresholds = n_comp_thresholds,
k_values = k_values,
percent_thresholds = percent_thresholds,
num_splits = num_splits
)
tuning_results_fisher$k
tuning_results_fisher$accuracy
tuning_results_fisher$n_components
tuning_results_fisher$threshold_value
tuning_results_fisher$distance_function
lppo_tuning_FISHER <- function(data, labels, num_persons_out, n_comp_thresholds,
k_values, percent_thresholds, num_splits,
distance_funcs = list(
"mahalanobis" = mahalanobis_distance,
"sse_mod" = sse_mod_distance,
"w_angle" = w_angle_distance
)) {
# Create objects
unique_persons <- unique(labels)
best_accuracy <- 0
best_k <- NULL
best_percent <- NULL
best_n_comp <- NULL
best_distance <- NULL
results_df <- data.frame()
# Cast labels to character to ensure consistent comparison
labels <- as.character(labels)
total_combinations <- length(n_comp_thresholds) * length(k_values) *
length(percent_thresholds) * length(distance_funcs) * num_splits
current_combination <- 0
for (split in 1:num_splits) {
cat(sprintf("Processing split %d/%d\n", split, num_splits))
# train/test split
split_seed <- 782 + split
split_data <- create_train_test_split(
data = data,
labels = labels,
num_persons_out = num_persons_out,
split_seed = split_seed
)
train_data <- split_data$train_data
train_labels <- split_data$train_labels
test_data <- split_data$test_data
test_labels <- split_data$test_labels
# FDA and projection
f_model <- Fisher(train_data, train_labels)
cumulative_variance <- cumsum(f_model$var_exp) #/sum(pca_model$var_exp)
n_comp_values <- sapply(n_comp_thresholds, function(th) {
min(which(cumulative_variance >= th))
})
n_comp_values <- unique(n_comp_values)
for (n_comp in n_comp_values) {
f_model_n <- list(vectors = f_model$vectors[, 1:n_comp],
mean = f_model$mean,
values = f_model$values[1:n_comp])
train_f <- project_fisher(train_data, f_model_n)
test_f <- project_fisher(test_data, f_model_n)
# K_NN tuning
for (dist_name in names(distance_funcs)) {
dist_func <- distance_funcs[[dist_name]]
for (k in k_values) {
for (percent_threshold in percent_thresholds) {
# Add error handling
tryCatch({
result <- knn_classifier(
train_data = train_f,
train_labels = train_labels,
test_data = test_f,
k = k,
percent_threshold = percent_threshold,
pca_model = f_model_n,
n_comp = n_comp,
distance_func = dist_func
)
predictions <- result$pred
# Calculate accuracy
# correct images labeled
correct_predictions <- sum(predictions == test_labels)
# Identify impostors
impostors <- names(table(test_labels)[table(test_labels)==6]) # all the images in the test set
correct_impostors <- sum(predictions== 0 & test_labels %in% impostors)
accuracy <- (correct_predictions + correct_impostors) / length(predictions)
# Check if threshold exists
if (is.null(result$thres) || length(result$thres) == 0) {
threshold_value <- NA
} else {
threshold_value <- result$thres
}
result_row <- data.frame(
split = split,
n_comp = n_comp,
k = k,
percent = percent_threshold,
distance = dist_name,
accuracy = accuracy,
var_explained = cumulative_variance[n_comp],
threshold_value = threshold_value
)
results_df <- rbind(results_df, result_row)
}, error = function(e) {
cat(sprintf("\nError with dist=%s, k=%d, percent=%.2f: %s\n",
dist_name, k, percent_threshold, e$message))
})
current_combination <- current_combination + 1
cat(sprintf("\rProgress: %d/%d combinations (%.2f%%)",
current_combination, total_combinations,
(current_combination / total_combinations) * 100))
}
}
}
}
}
# Calculate average results across splits
avg_results <- aggregate(
accuracy ~ n_comp + k + percent + distance,
data = results_df,
FUN = mean
)
# Find best parameters
best_idx <- which.max(avg_results$accuracy)
best_params <- avg_results[best_idx, ]
# Get the average threshold value for the best configuration
best_config_results <- results_df[
results_df$n_comp == best_params$n_comp &
results_df$k == best_params$k &
results_df$percent == best_params$percent &
results_df$distance == best_params$distance,
]
best_threshold <- mean(best_config_results$threshold_value, na.rm = TRUE)
# Calculate variance explained
best_var_exp <- mean(best_config_results$var_explained)
# Return only the essential information
return(list(
k = best_params$k,
percent_threshold = best_params$percent,
n_components = best_params$n_comp,
distance_function = best_params$distance,
accuracy = best_params$accuracy,
threshold_value = best_threshold,
variance_explained = best_var_exp,
results_df = results_df
))
}
tuning_results_fisher <- lppo_tuning_FISHER(
data = proj,
labels = labels,
num_persons_out = num_persons_out,
n_comp_thresholds = n_comp_thresholds,
k_values = k_values,
percent_thresholds = percent_thresholds,
num_splits = num_splits
)
average_distance <- tuning_results_fisher$avg_results %>%
group_by(distance) %>%
summarise(av_accuracy = mean(accuracy))
average_distance <- tuning_results_fisher$results_df
average_distance <- tuning_results_fisher$results_dfn_comp
average_distance
lppo_tuning_FISHER <- function(data, labels, num_persons_out, n_comp_thresholds,
k_values, percent_thresholds, num_splits,
distance_funcs = list(
"mahalanobis" = mahalanobis_distance,
"sse_mod" = sse_mod_distance,
"w_angle" = w_angle_distance
)) {
# Create objects
unique_persons <- unique(labels)
best_accuracy <- 0
best_k <- NULL
best_percent <- NULL
best_n_comp <- NULL
best_distance <- NULL
results_df <- data.frame()
# Cast labels to character to ensure consistent comparison
labels <- as.character(labels)
total_combinations <- length(n_comp_thresholds) * length(k_values) *
length(percent_thresholds) * length(distance_funcs) * num_splits
current_combination <- 0
for (split in 1:num_splits) {
cat(sprintf("Processing split %d/%d\n", split, num_splits))
# train/test split
split_seed <- 782 + split
split_data <- create_train_test_split(
data = data,
labels = labels,
num_persons_out = num_persons_out,
split_seed = split_seed
)
train_data <- split_data$train_data
train_labels <- split_data$train_labels
test_data <- split_data$test_data
test_labels <- split_data$test_labels
# FDA and projection
f_model <- Fisher(train_data, train_labels)
cumulative_variance <- cumsum(f_model$var_exp) #/sum(pca_model$var_exp)
n_comp_values <- sapply(n_comp_thresholds, function(th) {
min(which(cumulative_variance >= th))
})
n_comp_values <- unique(n_comp_values)
for (n_comp in n_comp_values) {
f_model_n <- list(vectors = f_model$vectors[, 1:n_comp],
mean = f_model$mean,
values = f_model$values[1:n_comp])
train_f <- project_fisher(train_data, f_model_n)
test_f <- project_fisher(test_data, f_model_n)
# K_NN tuning
for (dist_name in names(distance_funcs)) {
dist_func <- distance_funcs[[dist_name]]
for (k in k_values) {
for (percent_threshold in percent_thresholds) {
# Add error handling
tryCatch({
result <- knn_classifier(
train_data = train_f,
train_labels = train_labels,
test_data = test_f,
k = k,
percent_threshold = percent_threshold,
pca_model = f_model_n,
n_comp = n_comp,
distance_func = dist_func
)
predictions <- result$pred
# Calculate accuracy
# correct images labeled
correct_predictions <- sum(predictions == test_labels)
# Identify impostors
impostors <- names(table(test_labels)[table(test_labels)==6]) # all the images in the test set
correct_impostors <- sum(predictions== 0 & test_labels %in% impostors)
accuracy <- (correct_predictions + correct_impostors) / length(predictions)
# Check if threshold exists
if (is.null(result$thres) || length(result$thres) == 0) {
threshold_value <- NA
} else {
threshold_value <- result$thres
}
result_row <- data.frame(
split = split,
n_comp = n_comp,
k = k,
percent = percent_threshold,
distance = dist_name,
accuracy = accuracy,
var_explained = cumulative_variance[n_comp],
threshold_value = threshold_value
)
results_df <- rbind(results_df, result_row)
}, error = function(e) {
cat(sprintf("\nError with dist=%s, k=%d, percent=%.2f: %s\n",
dist_name, k, percent_threshold, e$message))
})
current_combination <- current_combination + 1
cat(sprintf("\rProgress: %d/%d combinations (%.2f%%)",
current_combination, total_combinations,
(current_combination / total_combinations) * 100))
}
}
}
}
}
# Calculate average results across splits
avg_results <- aggregate(
accuracy ~ n_comp + k + percent + distance,
data = results_df,
FUN = mean
)
# Find best parameters
best_idx <- which.max(avg_results$accuracy)
best_params <- avg_results[best_idx, ]
# Get the average threshold value for the best configuration
best_config_results <- results_df[
results_df$n_comp == best_params$n_comp &
results_df$k == best_params$k &
results_df$percent == best_params$percent &
results_df$distance == best_params$distance,
]
best_threshold <- mean(best_config_results$threshold_value, na.rm = TRUE)
# Calculate variance explained
best_var_exp <- mean(best_config_results$var_explained)
# Return only the essential information
return(list(
k = best_params$k,
percent_threshold = best_params$percent,
n_components = best_params$n_comp,
distance_function = best_params$distance,
accuracy = best_params$accuracy,
threshold_value = best_threshold,
variance_explained = best_var_exp,
avg_results = avg_results
))
}
tuning_results_fisher <- lppo_tuning_FISHER(
data = proj,
labels = labels,
num_persons_out = num_persons_out,
n_comp_thresholds = n_comp_thresholds,
k_values = k_values,
percent_thresholds = percent_thresholds,
num_splits = num_splits
)
average_distance <- tuning_results_fisher$avg_results %>%
group_by(distance) %>%
summarise(av_accuracy = mean(accuracy))
average_results <- tuning_results_fisher$avg_results %>%
group_by(distance, k, n_comp) %>%
summarise(av_accuracy = mean(accuracy)) %>%
arrange(desc(av_accuracy))
average_distance
average_results %>% head(10)
average_results %>% head(20)
# Define parameter grid for tuning
n_comp_thresholds <- c(0.80, 0.85, 0.90, 0.95)  # Cumulative variance thresholds for PCA
k_values <- c(1, 3, 5)  # Number of neighbors for KNN
percent_thresholds <- c(0.1, 0.2, 0.3)  # Thresholds for novel person detection
num_splits <- 5  # Number of cross-validation splits
num_persons_out <- 2  # Number of persons to leave out for testing
PCA_model = PCA(X,21)
proj = project_pca(X,PCA_model)[,1:21]
# Run the tuning function
source("function_loading.R")
tuning_results_fisher <- lppo_tuning_FISHER(
data = proj,
labels = labels,
num_persons_out = num_persons_out,
n_comp_thresholds = n_comp_thresholds,
k_values = k_values,
percent_thresholds = percent_thresholds,
num_splits = num_splits
)
tuning_results_fisher %>% saveRDS("results_fisher_tuning.RDS")
tuning_results_fisher <- readRDS("results_fisher_tuning.RDS")
average_distance <- tuning_results_fisher$avg_results %>%
group_by(distance) %>%
summarise(av_accuracy = mean(accuracy))
average_results <- tuning_results_fisher$avg_results %>%
group_by(distance, k, n_comp) %>%
summarise(av_accuracy = mean(accuracy)) %>%
arrange(desc(av_accuracy))
average_distance
average_results %>% head(10)
hist(predictions$between, xlim = c(0,1600), col = "lightblue",
main="Distances within, between and threshold")
hist(predictions$within, xlim = c(0,1600), col = "yellow", add=T )
abline(v=predictions$thres)
tuning_results_fisher$k
tuning_results_fisher$percent_threshold
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = tuning_results_fisher$percent_threshold,
pca_model = f_model,  n_comp = 6)
# Produce a full dataset PCA to use as our base Fisher data
pca_model <- PCA(X, results$n_components)
# Produce a full dataset PCA to use as our base Fisher data
pca_model <- PCA(X, results$n_components)
proj= project_pca(X,pca_model)
# Train test split
split_data <- create_train_test_split(data=proj, labels = labels, num_persons_out = 2,
split_seed = 782)
train_data <- split_data$train_data
train_labels <- split_data$train_labels
test_data <- split_data$test_data
test_labels <- split_data$test_labels
# Declare Fisher model
f_model = Fisher(train_data,train_labels)
# Projection of train and test
train_f <- project_fisher(train_data, f_model)
test_f <- project_fisher(test_data, f_model)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = tuning_results_fisher$percent_threshold,
pca_model = f_model,  n_comp = 6)
knn_classifier()
results$threshold_value
# Produce a full dataset PCA to use as our base Fisher data
pca_model <- PCA(X, results$n_components)
proj= project_pca(X,pca_model)
# Train test split
split_data <- create_train_test_split(data=proj, labels = labels, num_persons_out = 2,
split_seed = 782)
train_data <- split_data$train_data
train_labels <- split_data$train_labels
test_data <- split_data$test_data
test_labels <- split_data$test_labels
# Declare Fisher model
f_model = Fisher(train_data,train_labels)
# Projection of train and test
train_f <- project_fisher(train_data, f_model)
test_f <- project_fisher(test_data, f_model)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 519,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
#-------------------------------------------------------------------------------------
table(predictions$pred, test_labels)
impostors <- names(table(test_labels)[table(test_labels)==6]) # all the images in the test set
correct_predictions <- sum(predictions$pred == test_labels)
correct_impostors <- sum(predictions$pred == 0 & test_labels %in% impostors)
accuracy <- (correct_predictions + correct_impostors) / length(predictions$pred)
cat("accuracy:", accuracy)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 0.2,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 20,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 10,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 5,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
table(predictions$pred, test_labels)
impostors <- names(table(test_labels)[table(test_labels)==6]) # all the images in the test set
correct_predictions <- sum(predictions$pred == test_labels)
correct_impostors <- sum(predictions$pred == 0 & test_labels %in% impostors)
accuracy <- (correct_predictions + correct_impostors) / length(predictions$pred)
cat("accuracy:", accuracy)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 10,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
#-------------------------------------------------------------------------------------
table(predictions$pred, test_labels)
impostors <- names(table(test_labels)[table(test_labels)==6]) # all the images in the test set
correct_predictions <- sum(predictions$pred == test_labels)
correct_impostors <- sum(predictions$pred == 0 & test_labels %in% impostors)
accuracy <- (correct_predictions + correct_impostors) / length(predictions$pred)
cat("accuracy:", accuracy)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 8,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
#-------------------------------------------------------------------------------------
table(predictions$pred, test_labels)
impostors <- names(table(test_labels)[table(test_labels)==6]) # all the images in the test set
correct_predictions <- sum(predictions$pred == test_labels)
correct_impostors <- sum(predictions$pred == 0 & test_labels %in% impostors)
accuracy <- (correct_predictions + correct_impostors) / length(predictions$pred)
cat("accuracy:", accuracy)
hist(predictions$between, xlim = c(0,1600), col = "lightblue",
main="Distances within, between and threshold")
hist(predictions$within, xlim = c(0,1600), col = "yellow", add=T )
abline(v=predictions$thres)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 2,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
hist(predictions$between, xlim = c(0,1600), col = "lightblue",
main="Distances within, between and threshold")
hist(predictions$within, xlim = c(0,1600), col = "yellow", add=T )
abline(v=predictions$thres)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 4,
pca_model = f_model,  n_comp = 6,
)
distance_func=mahalanobis_distance)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 4,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
predictions <- knn_classifier(train_data = train_f, train_labels = train_labels,
test_data = test_data,   k = tuning_results_fisher$k,
percent_threshold = 6,
pca_model = f_model,  n_comp = 6,
distance_func=mahalanobis_distance)
